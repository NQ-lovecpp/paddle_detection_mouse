# 2.2 关于多卡训练

> **环境**: 2× Tesla T4 (15GB VRAM)，PaddlePaddle 2.5.1，CUDA 11.6，cuDNN 8.4  
> **PaddleDetection**: release/2.6  
> **更新日期**: 2026-02-07

---

## 1. 多卡训练的本质：数据并行（Data Parallelism）

多卡训练的核心思想是 **数据并行**：

```
                  ┌──── GPU 0: 前向 + 反向 → 本地梯度 ────┐
全局 Batch ──拆分─┤                                       ├── AllReduce ── 同步更新参数
                  └──── GPU 1: 前向 + 反向 → 本地梯度 ────┘
```

每张卡拿到一个 mini-batch 的子集，独立做前向和反向传播得到本地梯度，然后通过 **集合通信（Collective Communication）** 将梯度在所有卡之间同步，最终用聚合梯度更新模型参数。

关键公式：
- **等效 batch size** = `单卡 batch_size × GPU 数量`
- **线性缩放法则** (Linear Scaling Rule)：`new_lr = base_lr × (new_effective_bs / original_effective_bs)`

---

## 2. NCCL 库：GPU 集合通信的核心

### 2.1 什么是 NCCL

**NCCL**（NVIDIA Collective Communications Library）是 NVIDIA 官方提供的 GPU 间集合通信库。它是目前多 GPU 训练的 **事实标准通信后端**，被 PaddlePaddle、PyTorch、TensorFlow 等主流框架共同使用。

### 2.2 NCCL 支持的通信原语

| 原语 | 描述 | 训练中的用途 |
|------|------|-------------|
| **AllReduce** | 所有节点的数据进行归约（如求和），结果广播给所有节点 | **梯度同步**（最核心操作） |
| **Broadcast** | 一个节点的数据广播给所有节点 | 模型参数初始化 |
| **AllGather** | 所有节点收集彼此的数据 | 特征聚合 |
| **ReduceScatter** | 归约后将结果分散到各节点 | 梯度分片更新 |
| **Send / Recv** | 点对点通信 | Pipeline 并行 |

### 2.3 NCCL 的通信拓扑

NCCL 会自动检测 GPU 互联拓扑，选择最优的通信路径：

```
┌─────────────────────────────────────────────────────────────┐
│                    单机多卡（本项目场景）                       │
│                                                             │
│   GPU 0 ◄══════════ PCIe Bus ══════════► GPU 1              │
│          （PCIe 3.0/4.0, ~16GB/s 单向）                      │
│                                                             │
│   如果有 NVLink:                                             │
│   GPU 0 ◄═══════ NVLink ════════► GPU 1                     │
│          （NVLink 2.0: ~150GB/s 双向）                       │
│          （NVLink 3.0: ~300GB/s 双向）                       │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                      多机多卡                                │
│                                                             │
│   Machine A                     Machine B                   │
│  ┌─────────┐                   ┌─────────┐                  │
│  │ GPU0 GPU1│ ←── InfiniBand ──→│ GPU0 GPU1│                │
│  │ GPU2 GPU3│     / TCP/IP      │ GPU2 GPU3│                │
│  └─────────┘                   └─────────┘                  │
│                                                             │
│  InfiniBand: ~200Gbps (RDMA, 零拷贝)                        │
│  TCP/IP (Socket): ~10-100Gbps (需要 CPU 参与)                │
└─────────────────────────────────────────────────────────────┘
```

### 2.4 PCIe 总线 vs Socket 通信

在多卡训练中，GPU 之间的梯度同步走什么"路"至关重要：

| 通信方式 | 带宽 | 延迟 | 适用场景 | 配置方式 |
|---------|------|------|---------|---------|
| **PCIe 总线** | ~16GB/s (PCIe 3.0) / ~32GB/s (PCIe 4.0) | 低 | 单机多卡（同一 PCIe 交换机下） | NCCL 自动选择 |
| **NVLink** | 150-600 GB/s | 极低 | 高端服务器（A100/H100） | NCCL 自动选择 |
| **Socket (TCP/IP)** | ~10-100 Gbps | 较高 | 多机多卡 | `NCCL_SOCKET_IFNAME=eth0` |
| **InfiniBand (RDMA)** | ~200 Gbps | 低 | 数据中心多机多卡 | 需要硬件支持 |

#### 本项目的情况

我们的 2× Tesla T4 大概率走 **PCIe 总线** 通信（T4 不支持 NVLink）。验证方式：

```bash
# 查看 GPU 拓扑
nvidia-smi topo -m

# 如果输出类似：
#        GPU0  GPU1
# GPU0    X    PHB    ← PHB 表示通过 PCIe Host Bridge 连接
# GPU1   PHB    X
```

拓扑标识含义：

| 标识 | 含义 | 带宽 |
|------|------|------|
| **X** | 自身 | — |
| **NV#** | NVLink | 最高 |
| **PIX** | 同一 PCIe 交换机 | 高 |
| **PHB** | 同一 PCIe Host Bridge | 中 |
| **NODE** | 同一 NUMA 节点 | 中低 |
| **SYS** | 跨 NUMA 节点 | 低 |

### 2.5 NCCL 的 AllReduce 算法

NCCL 的 AllReduce 实现使用 **Ring AllReduce** 或 **Tree AllReduce** 算法：

#### Ring AllReduce（默认）

将 N 个 GPU 组成一个逻辑环：

```
  ┌─── GPU 0 ───┐
  │              │
  ▼              ▲
GPU 1          GPU 3
  │              ▲
  ▼              │
  └─── GPU 2 ───┘

步骤 1 (Reduce-Scatter): 
  每个 GPU 把梯度切成 N 份，沿环传递并累加
  需要 N-1 步，每步传输 data_size/N 的数据

步骤 2 (AllGather): 
  把归约完成的分片沿环传递给所有节点
  需要 N-1 步

总通信量: 2 × (N-1)/N × data_size ≈ 2 × data_size（当 N 较大时）
```

**关键优势**：通信量与 GPU 数量几乎无关，只与模型大小相关。这意味着增加 GPU 数量可以获得接近线性的加速比。

#### Tree AllReduce

适用于大量 GPU（如 256+）的场景，利用树状拓扑减少延迟。NCCL 会根据实际环境自动选择最优算法。

### 2.6 NCCL 环境变量调优

```bash
# --- 基础配置 ---
# 指定使用的网络接口（多机训练时）
export NCCL_SOCKET_IFNAME=eth0

# 启用 NCCL debug 日志（调试用）
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# --- 性能调优 ---
# 设置 NCCL 使用的 buffer 大小（默认 4MB，可以增大到 16MB 或 32MB）
export NCCL_BUFFSIZE=16777216

# 线程数设置
export NCCL_NTHREADS=512

# --- 通信后端选择 ---
# 强制使用 InfiniBand（如果硬件支持）
export NCCL_IB_DISABLE=0

# 强制使用 Socket（当 IB 不可用时）
export NCCL_IB_DISABLE=1

# --- 多机训练端口 ---
export FLAGS_START_PORT=17000
```

---

## 3. PaddlePaddle 的多卡启动方式

### 3.1 paddle.distributed.launch（推荐）

```bash
# 单机双卡
python -m paddle.distributed.launch --gpus 0,1 \
    tools/train.py -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    --eval

# 等价的 fleetrun 命令
fleetrun --selected_gpu 0,1 \
    tools/train.py -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    --eval
```

`paddle.distributed.launch` 的内部机制：
1. **fork** 出 N 个进程（N = GPU 数量）
2. 每个进程绑定一张 GPU（通过 `CUDA_VISIBLE_DEVICES`）
3. 进程间通过 NCCL 进行梯度同步
4. 使用 **gloo** 或 **NCCL** 作为进程组的通信后端

### 3.2 多机多卡

```bash
# 在每台机器上都运行相同的命令
ip_list="192.168.1.100,192.168.1.101"  # 所有机器的 IP

fleetrun \
    --ips=${ip_list} \
    --selected_gpu 0,1 \
    tools/train.py -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    --eval
```

**前置要求**：
- 各机器间免密 SSH
- 各机器上的代码、数据路径完全一致
- 网络端口开放（默认 6170+）
- 建议设置 `export FLAGS_START_PORT=17000`

### 3.3 Fleet API（高级用法）

PaddlePaddle 还提供了 Fleet API 用于更灵活的分布式训练控制：

```bash
# 使用 fleet 模式
python -m paddle.distributed.launch --gpus 0,1 \
    tools/train.py -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    --fleet \
    --eval
```

`--fleet` 标志会启用 PaddlePaddle 的 Fleet 分布式训练引擎，支持：
- 自动混合精度（AMP）
- 梯度累积
- 参数服务器模式（PS Mode）

---

## 4. 本项目的多卡训练实战

### 4.1 确认 GPU 拓扑

```bash
nvidia-smi topo -m
```

### 4.2 单机双卡训练命令

```bash
cd /hy-tmp/paddle_detection_mouse/PaddleDetection-release-2.6

export CUDA_VISIBLE_DEVICES=0,1

# 双卡训练 — 注意学习率需要按线性缩放法则调整
python -m paddle.distributed.launch --gpus 0,1 \
    tools/train.py \
    -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    -o \
    LearningRate.base_lr=0.0025 \
    save_dir=output/yolov3_mouse_other_voc_2gpu \
    --eval \
    --use_vdl=true \
    --vdl_log_dir=output/yolov3_mouse_other_voc_2gpu/vdl_log
```

### 4.3 监控通信性能

```bash
# 训练时观察 NCCL 日志
NCCL_DEBUG=INFO python -m paddle.distributed.launch --gpus 0,1 \
    tools/train.py -c configs/yolov3/yolov3_my_dog_mouse_voc.yml \
    -o epoch=1 2>&1 | grep -i "nccl\|ring\|tree\|pcie\|nvlink"
```

### 4.4 学习率线性缩放速查表

| GPU 数 | 单卡 BS | 等效 BS | base_lr | 说明 |
|--------|---------|---------|---------|------|
| 1 | 8 | 8 | 0.00125 | 标准单卡 |
| 2 | 8 | 16 | 0.0025 | 本项目推荐 |
| 2 | 16 | 32 | 0.005 | 大 batch 实验 |
| 4 | 8 | 32 | 0.005 | 4 卡场景 |
| 8 | 8 | 64 | 0.01 | 原始配置 |

> **注意**：当等效 batch size 较大时（>64），线性缩放可能不再精确，建议适当增加 warmup 步数。PaddleDetection 的分布式训练文档指出，GPU 卡数过多时精度会有 ~1% 的损失，可通过增加 warmup 或迭代轮数补偿。

---

## 5. 常见问题排查

### 5.1 NCCL 初始化失败

```
RuntimeError: NCCL error: unhandled cuda error
```

**解决方案**：
```bash
# 降低 NCCL 的 P2P 级别
export NCCL_P2P_LEVEL=NVL   # 或 PIX, PHB, NODE

# 禁用 P2P（极端情况）
export NCCL_P2P_DISABLE=1
```

### 5.2 多卡训练速度没有加快

可能原因：
1. **数据加载成为瓶颈** → 增加 `worker_num`
2. **模型太小，通信开销占比大** → YOLOv3-MobileNetV1 较轻量，2 卡加速比约 1.5-1.8x
3. **PCIe 带宽不足** → T4 通过 PCIe 通信，对大 batch 略有影响

### 5.3 GPU 显存不均匀

```bash
# 通过环境变量均衡分配
export FLAGS_fraction_of_gpu_memory_to_use=0.92
export FLAGS_eager_delete_tensor_gb=0.0
```

---

## 参考资料

- [PaddlePaddle 分布式训练文档](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/06_distributed_training/index_cn.html)
- [NCCL 官方文档](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
- [PaddleDetection 分布式训练教程](../../PaddleDetection-release-2.6/docs/tutorials/DistributedTraining_cn.md)
- [Fleet API 快速开始](https://fleet-x.readthedocs.io/en/latest/)
