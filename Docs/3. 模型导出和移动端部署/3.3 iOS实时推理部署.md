# 3.3 iOS 实时推理部署

> 本文记录将 PicoDet-S 鼠标检测模型部署到 iPhone 并实现实时摄像头推理的完整过程，包括性能瓶颈排查和最终解决方案。

---

## 目录

1. [项目概述](#1-项目概述)
2. [技术选型](#2-技术选型)
3. [性能瓶颈排查过程](#3-性能瓶颈排查过程)
4. [最终架构：全原生推理管线](#4-最终架构全原生推理管线)
5. [核心代码详解](#5-核心代码详解)
6. [模型转换与打包](#6-模型转换与打包)
7. [运行效果](#7-运行效果)
8. [踩坑记录](#8-踩坑记录)

---

## 1. 项目概述

**目标**：在 iPhone 13 Pro 上以摄像头视频流为输入，实时检测实验鼠，并在画面上叠加 bounding box。

**最终效果**：
- 推理帧率：**~14 FPS**（PicoDet-S 320×320，CoreML ANE 加速）
- 推理延迟：**~70ms / 帧**（含快照、预处理、推理、NMS）
- 模型大小：4.4 MB（vs 原 YOLOv3 92 MB）

**技术栈**：
- 前端：React Native（TypeScript）
- 摄像头：`react-native-vision-camera`
- 推理引擎：ONNX Runtime C API（通过 `onnxruntime-c` CocoaPod）
- 硬件加速：CoreML（Apple Neural Engine / ANE）
- 图像处理：Accelerate.framework（vDSP）

---

## 2. 技术选型

### 2.1 为什么用 ONNX 而不是 CoreML 原生格式？

PaddleDetection 训练的模型可以通过 `paddle2onnx` 转为标准 ONNX 格式，再由 ONNX Runtime 的 CoreML Execution Provider 在 iPhone 的 ANE 上运行，无需手动转换为 `.mlpackage`。

```
PaddleDetection 模型
    ↓ paddle2onnx
ONNX 模型 (.onnx)
    ↓ ONNX Runtime CoreML EP
Apple Neural Engine (ANE)
```

### 2.2 为什么选 PicoDet-S 而不是 YOLOv3？

| 模型 | 大小 | iPhone CPU 推理 | CoreML ANE 推理 |
|------|------|-----------------|-----------------|
| YOLOv3 + MobileNetV1 | 92 MB | ~2000ms | 不兼容（QLinearMul 报错）|
| PicoDet-S 320×320 | **4.4 MB** | ~500ms | **~50ms** |

PicoDet 是百度飞桨专为移动端设计的轻量检测器，结构更适合 CoreML。

### 2.3 为什么用 ORT C API 而不是 `onnxruntime-react-native`？

`onnxruntime-react-native` 提供了 JavaScript 调用 ORT 的能力，但存在两个问题：

1. **1.2 MB bridge 传输**：每帧需要把 320×320×3×4 = 1.2 MB 的 Float32 数组从 Native Swift 传给 JS（base64 编码后约 1.6 MB），再由 JS 创建 OrtTensor，这在 React Native bridge 上极慢（约 100-200ms）。

2. **CoreML EP 不可靠**：JS 侧调用的 CoreML EP 有时会静默回退到 CPU，无法确保 ANE 加速。

直接使用 `onnxruntime-c` CocoaPod 的 C API，在 Objective-C++ 中完成整个管线，只有极小的检测结果 JSON（通常 < 1 KB）通过 bridge 返回。

---

## 3. 性能瓶颈排查过程

### 3.1 初始实现（~1 FPS）

最初用纯 JavaScript 做预处理：

```
takeSnapshot → [bridge] → JS: jpeg-js 解码 + 手写 resize/normalize → [ORT JS] → 结果
预处理耗时：约 3000-16000ms（纯 JS，单线程）
```

### 3.2 Swift 原生预处理（~2 FPS）

将预处理移入 Swift，用 Accelerate.framework：

```
takeSnapshot → [bridge] → ImagePreprocessor.swift → [bridge 1.2MB float] → ORT JS 推理
预处理：~5-30ms（Swift/Accelerate）
bridge传输：~100ms
ORT CPU 推理：~500ms
总计：~600ms → 约 2 FPS（受 500ms setInterval 硬性限制）
```

发现了两个额外问题：
- 代码中 `setInterval(runInference, 500)` 直接把 FPS 上限锁在了 2
- `onnxruntime-react-native` 的 CoreML EP 不生效（静默回退 CPU）

### 3.3 全原生 C++ 管线（~14 FPS） ✅

把整个管线搬进一个 Objective-C++ 模块，使用 ORT C API + CoreML：

```
takeSnapshot → [bridge] → MouseDetector.mm:
                              resize(CoreGraphics) → normalize(Accelerate) → ORT C API + CoreML ANE
                          → [bridge < 1KB JSON] → 渲染 bounding box
```

| 阶段 | 耗时 |
|------|------|
| 快照 (`takeSnapshot`) | ~15ms |
| 图像缩放 (CoreGraphics) | ~5ms |
| 归一化 (vDSP/Accelerate) | ~3ms |
| **ORT C API + CoreML ANE 推理** | **~40-50ms** |
| NMS（C++ 手写） | <1ms |
| Bridge 返回 JSON | <1ms |
| **总计** | **~70ms → 14 FPS** |

---

## 4. 最终架构：全原生推理管线

### 4.1 整体架构图

```
┌─────────────────────────────────────────────────────┐
│                React Native JS 层                    │
│                                                     │
│  DetectionScreen.tsx                                │
│  ┌─────────────────────────────────────────────┐   │
│  │  async 推理循环（无 setInterval 延迟）          │   │
│  │  while (loopActive) {                       │   │
│  │    photo = await camera.takeSnapshot()      │   │
│  │    result = await MouseDetector.detect(path)│   │
│  │    renderBoundingBoxes(result.detections)   │   │
│  │  }                                          │   │
│  └─────────────────────────────────────────────┘   │
└──────────────────────┬──────────────────────────────┘
                       │ RN Bridge（仅传文件路径 + 检测结果 JSON）
┌──────────────────────▼──────────────────────────────┐
│              MouseDetector.mm (Objective-C++)         │
│                                                     │
│  1. UIImage 加载 JPEG 文件                           │
│  2. CoreGraphics → 缩放到 320×320 RGBA               │
│  3. Accelerate vDSP → UInt8 RGBA → Float32 CHW      │
│     （ImageNet 归一化：mean/std，去交错）             │
│  4. ORT C API → CreateTensorWithDataAsOrtValue       │
│  5. CoreML EP (COREML_FLAG_CREATE_MLPROGRAM)         │
│     → Apple ANE 推理                                 │
│  6. 解析 boxes[1,2125,4] + scores[1,2,2125]         │
│  7. Per-class Greedy NMS                            │
│  8. 返回检测框数组（JSON）                            │
└──────────────────────┬──────────────────────────────┘
                       │ ORT C API
┌──────────────────────▼──────────────────────────────┐
│         onnxruntime-c (CocoaPod xcframework)         │
│         CoreML Execution Provider                   │
│         Apple Neural Engine (ANE)                   │
└─────────────────────────────────────────────────────┘
```

### 4.2 目录结构

```
Mobile_Deployment/MouseDetectionApp/
├── ios/
│   ├── MouseDetectionApp/
│   │   ├── AppDelegate.swift
│   │   ├── MouseDetector.mm          ← 核心：全原生推理模块
│   │   ├── ImagePreprocessor.swift   ← 旧版（已不使用，保留参考）
│   │   ├── ImagePreprocessor.m       ← 旧版桥接头
│   │   └── Resources/
│   │       ├── picodet_s_320_mouse_L1_nonms.onnx  ← 模型文件
│   │       └── label_list.txt
│   └── Podfile
├── src/
│   ├── screens/
│   │   └── DetectionScreen.tsx       ← 主屏幕，调用原生模块
│   └── components/
│       ├── BoundingBoxOverlay.tsx    ← 边框叠加层
│       ├── DebugConsole.tsx          ← 调试日志面板
│       └── ThresholdSlider.tsx       ← 置信度阈值滑块
└── package.json
```

---

## 5. 核心代码详解

### 5.1 MouseDetector.mm — 原生推理模块

这是整个方案的核心，用 Objective-C++（`.mm` 文件）写成，可以同时调用 ORT C API（C++）和 iOS 框架（ObjC）。

**关键 import：**
```objc
#import <UIKit/UIKit.h>           // UIImage 加载图片
#import <Accelerate/Accelerate.h> // vDSP 向量化归一化
#import <onnxruntime/onnxruntime_c_api.h>      // ORT C API
#import <onnxruntime/coreml_provider_factory.h> // CoreML EP
#include <vector>     // std::vector for NMS
#include <algorithm>  // std::sort
```

**初始化（加载模型，启用 CoreML）：**
```objc
// 创建 ORT Session Options
OrtSessionOptions* opts;
_api->CreateSessionOptions(&opts);
_api->SetSessionGraphOptimizationLevel(opts, ORT_ENABLE_ALL);

// 启用 CoreML Execution Provider（MLProgram 格式，充分利用 ANE）
// 不支持的算子自动回退 CPU，不会报错
OrtSessionOptionsAppendExecutionProvider_CoreML(opts, COREML_FLAG_CREATE_MLPROGRAM);

// 从 .onnx 文件创建推理 Session
const char* path = [modelPath UTF8String];
_api->CreateSession(_env, path, opts, &_session);
```

**图像预处理（CoreGraphics + Accelerate）：**
```objc
// 1. 缩放到 320×320，像素写入 RGBA buffer
std::vector<uint8_t> pixels(320 * 320 * 4);
CGContextRef ctx = CGBitmapContextCreate(pixels.data(), 320, 320, 8, 320*4,
                                         CGColorSpaceCreateDeviceRGB(),
                                         kCGImageAlphaNoneSkipLast);
CGContextSetInterpolationQuality(ctx, kCGInterpolationMedium);
CGContextDrawImage(ctx, CGRectMake(0, 0, 320, 320), uiImage.CGImage);

// 2. RGBA UInt8 → CHW Float32，同时做 ImageNet 归一化
// vDSP_vfltu8 用 stride=4 直接从 RGBA 提取各通道，一次完成去交错和类型转换
std::vector<float> chw(3 * 320 * 320);
vDSP_vfltu8(pixels.data() + 0, 4, chw.data(),              1, 320*320); // R
vDSP_vfltu8(pixels.data() + 1, 4, chw.data() + 320*320,    1, 320*320); // G
vDSP_vfltu8(pixels.data() + 2, 4, chw.data() + 2*320*320,  1, 320*320); // B

// vDSP_vsmsa 做 fused multiply-add：pixel = pixel * scale + offset
// 等价于：(pixel / 255 - mean) / std，全向量化
for (int c = 0; c < 3; c++) {
    float scale  = 1.0f / (255.0f * kStd[c]);   // ImageNet std
    float offset = -kMean[c] / kStd[c];          // ImageNet mean
    vDSP_vsmsa(chw.data() + c*320*320, 1, &scale, &offset,
               chw.data() + c*320*320, 1, 320*320);
}
```

**ORT C API 推理：**
```objc
// 将 CHW float 数组包装为 OrtValue（零拷贝，直接引用 chw.data()）
int64_t shape[] = {1, 3, 320, 320};
OrtValue* inputTensor;
_api->CreateTensorWithDataAsOrtValue(
    _memInfo,
    chw.data(), chw.size() * sizeof(float),
    shape, 4,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,
    &inputTensor);

// 运行推理
const char* inputNames[]  = {"image"};
const char* outputNames[] = {"boxes", "scores"};
OrtValue* outputs[2] = {nullptr, nullptr};
_api->Run(_session, nullptr,
          inputNames, &inputTensor, 1,
          outputNames, 2, outputs);

// 直接读取输出指针（零拷贝）
float* boxData;   _api->GetTensorMutableData(outputs[0], (void**)&boxData);
float* scoreData; _api->GetTensorMutableData(outputs[1], (void**)&scoreData);
```

**输出格式（PicoDet-S nonms 模型）：**

| 张量名 | 形状 | 含义 |
|--------|------|------|
| `boxes` | [1, 2125, 4] | 各 anchor 预测框，xyxy 格式，坐标在 320px 空间内 |
| `scores` | [1, 2, 2125] | 各 anchor 各类别置信度，channels-first |

2125 = 40×40 + 20×20 + 10×10 + 5×5（PicoDet 4 级 FPN）

**Per-class Greedy NMS（C++）：**
```cpp
// 按类别分组 → 按置信度降序排列 → 贪心抑制
for (int cls = 0; cls < kNumClasses; cls++) {
    std::vector<Candidate*> cc;
    for (auto& c : candidates) if (c.classId == cls) cc.push_back(&c);
    std::sort(cc.begin(), cc.end(), [](auto* a, auto* b){ return a->score > b->score; });

    std::vector<bool> suppressed(cc.size(), false);
    for (size_t i = 0; i < cc.size(); i++) {
        if (suppressed[i]) continue;
        // 保留 cc[i]，抑制与其 IoU > 0.5 的框
        for (size_t j = i+1; j < cc.size(); j++) {
            if (!suppressed[j] && iou(cc[i], cc[j]) > 0.5f)
                suppressed[j] = true;
        }
    }
}
```

### 5.2 DetectionScreen.tsx — 前端调用

**初始化（加载模型）：**
```typescript
const { MouseDetector } = NativeModules;

useEffect(() => {
    const modelPath = `${RNFS.MainBundlePath}/picodet_s_320_mouse_L1_nonms.onnx`;
    MouseDetector.initialize(modelPath);  // 异步，CoreML 编译模型（首次较慢）
}, []);
```

**推理循环（替代 setInterval，全速运行）：**
```typescript
// 不用 setInterval，用 async 循环，每帧推理完成后立即开始下一帧
(async () => {
    while (loopActiveRef.current) {
        const photo = await cameraRef.current.takeSnapshot({ quality: 50 });

        // 一次 bridge 调用完成全部工作，返回极小 JSON
        const result = await MouseDetector.detect(photo.path, threshold);

        // result.detections: [{classId, className, confidence, x1, y1, x2, y2}]
        // result.timings: "resize=5ms norm=3ms infer=48ms nms=0ms total=56ms"
        setDetections(result.detections);

        await RNFS.unlink(photo.path); // 清理临时文件
    }
})();
```

### 5.3 Bridge 数据量对比

| 方案 | 每帧 Bridge 传输量 | 用途 |
|------|-------------------|------|
| 旧：JS 预处理 + ORT JS | 1.2 MB Float32（base64 1.6MB） | 预处理结果传给 ORT |
| 新：MouseDetector.mm | **< 1 KB JSON** | 检测结果（几个框） |

---

## 6. 模型转换与打包

### 6.1 从 PaddleDetection 导出 PicoDet ONNX

```bash
# 在服务器上训练完成后，导出 PicoDet-S
python tools/export_model.py \
    -c configs/picodet/picodet_s_320_coco_lcnet.yml \
    --output_dir=./inference_model \
    -o weights=output/picodet_mouse/best_model.pdparams

# 转为 ONNX（nonms 模式：NMS 不包含在模型内，由 App 自己做）
paddle2onnx \
    --model_dir inference_model/picodet_s_320_mouse \
    --model_filename model.pdmodel \
    --params_filename model.pdiparams \
    --opset_version 11 \
    --save_file picodet_s_320_mouse_L1_nonms.onnx \
    --without_nms True
```

> **为什么用 `--without_nms`？**  
> 包含 NMS 的 ONNX 算子（如 `NonMaxSuppression`）在 CoreML 上兼容性差。去掉 NMS 后由 App 端 C++ 手写 Greedy NMS，更高效且无兼容性问题。

### 6.2 加入 Xcode 项目

1. 将 `.onnx` 文件放入 `ios/MouseDetectionApp/Resources/`
2. 在 Xcode 的 `project.pbxproj` 中注册（已在代码中完成，见 `PBXBuildFile` 和 `PBXResourcesBuildPhase`）
3. 或者直接在 Xcode 中拖入并勾选"Add to target: MouseDetectionApp"

### 6.3 CocoaPods 配置

`onnxruntime-c` 由 `onnxruntime-react-native` 的 podspec 自动引入，无需手动添加。验证已安装：

```bash
ls ios/Pods/onnxruntime-c/onnxruntime.xcframework/
# 应该看到: ios-arm64/  ios-arm64_x86_64-simulator/  macos-arm64_x86_64/
```

---

## 7. 运行效果

### 7.1 启动应用

```bash
# 编译并部署到 iPhone（替换成你的设备 UDID）
npx react-native run-ios --device "00008110-001168A13C86401E"

# 另开终端，启动远程日志服务（在 Mac 端看调试日志）
cd Mobile_Deployment/MouseDetectionApp
node log-server.js
```

首次运行时，CoreML 会编译 ONNX 模型为 MLProgram（约 10-30 秒），编译结果缓存在设备上，后续启动秒开。

### 7.2 Debug 日志格式

Debug Console 每 5 帧输出一次详细时序：

```
snap=15ms resize=5ms norm=3ms infer=48ms nms=0ms total=71ms
✅ [0] mouse conf=87.3% bbox=(120,45,380,290)
```

### 7.3 性能数据（iPhone 13 Pro）

| 指标 | 数值 |
|------|------|
| 推理帧率 | ~14 FPS |
| 快照延迟 | ~15ms |
| CoreML ANE 推理 | ~40-50ms |
| 总帧时间 | ~70ms |
| 模型内存占用 | ~15MB |

---

## 8. 踩坑记录

### 8.1 `onnxruntime-react-native` CoreML EP 静默回退 CPU

**现象**：在 `InferenceSession.create(path, { executionProviders: ['coreml', 'cpu'] })` 后，推理速度和纯 CPU 一样慢（~500ms）。

**原因**：`onnxruntime-react-native` 的 CoreML EP 在某些模型结构下无法完整代理所有算子，回退到 CPU 执行，且不报错。

**解法**：使用 ORT C API（`onnxruntime-c` CocoaPod）直接调用 CoreML EP，在 ObjC++ 层可以捕获到完整的错误信息和回退原因。

### 8.2 YOLOv3 INT8 量化模型不兼容

**现象**：`QLinearMul node error`

**原因**：`onnxruntime-react-native` 当前版本不支持 QDQ（Quantize-Dequantize）格式的 INT8 算子。

**解法**：换用 PicoDet-S FP32 模型（4.4 MB），体积更小且 CoreML 兼容。

### 8.3 `takeSnapshot` 需要 `video={true}`

**现象**：`capture/snapshot-failed: Failed to take a Snapshot of the Preview View`

**原因**：`react-native-vision-camera` 的 `takeSnapshot` 在 iOS 上需要 video pipeline 开启。

**解法**：
```tsx
<Camera ref={cameraRef} photo={true} video={true} ... />
```

### 8.4 setInterval 导致 FPS 上限

**现象**：即使推理只需 200ms，FPS 始终不超过 2（= 1000ms / 500ms）

**原因**：`setInterval(runInference, 500)` 是固定间隔，不管上次推理是否完成或快慢。

**解法**：改为 `while` + `await` 的异步循环，推理完成后立即开始下一帧，取消人工延迟。

### 8.5 React Native Bridge 1.2 MB 传输瓶颈

**现象**：Native 预处理快（5ms），但 JS 侧 `Buffer.from(base64)` 解码慢（约 100ms），且 bridge 序列化本身 50-150ms。

**原因**：320×320×3×4 字节 = 1.2 MB 的 Float32 数组，以 base64 字符串（1.6 MB）形式通过 React Native bridge 传递，bridge 底层是 JSON 序列化，传大体积数据本来就慢。

**解法**：把 ORT 推理也搬进原生模块，数据完全在 C++ 内流动，bridge 只传检测结果（< 1 KB JSON）。

---

## 参考资料

- [ONNX Runtime C API 文档](https://onnxruntime.ai/docs/api/c/)
- [CoreML Execution Provider 配置说明](https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html)
- [PicoDet 官方文档](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.6/configs/picodet)
- [paddle2onnx 使用说明](https://github.com/PaddlePaddle/Paddle2ONNX)
- [react-native-vision-camera](https://react-native-vision-camera.com/)
- [Accelerate.framework vDSP 参考](https://developer.apple.com/documentation/accelerate/vdsp)
